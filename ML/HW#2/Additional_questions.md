1. Как влияет размер шага eta на сходимость алгоритма?

Слишком маленький eta (например, 0.001):

· Алгоритм сходится очень медленно
· Требуется много итераций для достижения минимума
· Риск застревания в локальных минимумах снижается

Оптимальный eta (например, 0.01-0.1):

· Алгоритм эффективно сходится к минимуму
· Баланс между скоростью сходимости и стабильностью

Слишком большой eta (например, 1.0):

· Алгоритм может расходиться (значение loss растет)
· Может "перепрыгивать" через минимум
· Нестабильное обучение

2. Что произойдет, если выбрать слишком маленькое или слишком большое количество итераций n_iterations?

Слишком мало итераций:

· Модель не успеет сойтись к оптимальным весам
· Недообучение
· Низкая точность на тренировочных и тестовых данных

Слишком много итераций:

· Переобучение - модель запоминает шум в данных
· Дольше время обучения
· Может ухудшиться обобщающая способность
· Риск переобучения особенно высок при маленьком размере выборки

Оптимальное количество:

· Использовать раннюю остановку
· Мониторинг функции потерь на валидационной выборке
· Останавливать, когда loss перестает уменьшаться

3. Почему логистическая регрессия подходит для задач только бинарной классификации?

Логистическая регрессия изначально разработана для бинарной классификации потому что:

1. Сигмоидная функция на выходе дает вероятность принадлежности к одному классу, вероятность второго класса вычисляется автоматически
3. Выход всегда в диапазоне [0, 1], что идеально для вероятностей
